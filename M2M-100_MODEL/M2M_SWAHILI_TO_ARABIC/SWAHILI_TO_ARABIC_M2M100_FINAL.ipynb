{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoZOYLhJg7Vz"
      },
      "source": [
        "# Installing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zr_4rVKGjrBC"
      },
      "outputs": [],
      "source": [
        "! pip install sentencepiece -q\n",
        "! pip install wandb\n",
        "import numpy\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "\n",
        "import wandb\n",
        "\n",
        "wandb.login()\n",
        "\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.enabled = False\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "set_seed(7)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!mkdir checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYmIDW5fN7yS"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/pytorch/fairseq -q\n",
        "%cd fairseq\n",
        "!pip uninstall numpy -q -y\n",
        "!pip install wandb -q\n",
        "!pip install --editable ./ -q\n",
        "%cd ..\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UldcvGCPOEbv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.get_device_name(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSA3MTlTOOWq"
      },
      "outputs": [],
      "source": [
        "!wget -qq \"https://dl.fbaipublicfiles.com/m2m_100/spm.128k.model\"\n",
        "!wget -qq \"https://dl.fbaipublicfiles.com/m2m_100/data_dict.128k.txt\"\n",
        "!wget -qq \"https://dl.fbaipublicfiles.com/m2m_100/model_dict.128k.txt\"\n",
        "!wget -qq \"https://dl.fbaipublicfiles.com/m2m_100/language_pairs_small_models.txt\"\n",
        "!wget     \"https://dl.fbaipublicfiles.com/m2m_100/418M_last_checkpoint.pt\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7dKphFchJDd"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMzE13yKxbHi"
      },
      "outputs": [],
      "source": [
        "!unzip /content/DATA.zip\n",
        "!unzip /content/final_split.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zr_SWTJ1xeN8"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/final_training_set.csv\")\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8t5hmEfxnme"
      },
      "outputs": [],
      "source": [
        "#prepare the training set\n",
        "PATH_TO_DATASET = \"/content/\"  #Where you stored the dataset\n",
        "\n",
        "train = pd.read_csv(os.path.join(PATH_TO_DATASET, \"final_training_set.csv\"))\n",
        "train.columns = [\"Swahili\",\"Arabic\"]\n",
        "\n",
        "#Remove any possible duplicates\n",
        "train = train.drop_duplicates(subset=[\"Arabic\", \"Swahili\"])\n",
        "\n",
        "#Lowercase and remove trailing spaces\n",
        "train[\"Arabic\"] = train.apply(lambda x: (x.Arabic).strip().lower(), axis=1)\n",
        "train[\"Swahili\"] = train.Swahili.apply(lambda x: x.lower())\n",
        "\n",
        "train = train[[ \"Swahili\",\"Arabic\" ]]\n",
        "train.columns = [\"input_text\", \"target_text\"]\n",
        "\n",
        "\n",
        "#prepare the test set\n",
        "validation = pd.read_csv(os.path.join(PATH_TO_DATASET, \"final_test_set.csv\"))\n",
        "validation.columns = [\"Swahili\",\"Arabic\"]\n",
        "#Remove any possible duplicates\n",
        "validation = validation.drop_duplicates(subset=[\"Arabic\", \"Swahili\"])\n",
        "\n",
        "#Lowercase and remove trailing spaces\n",
        "validation[\"Arabic\"] = validation.apply(lambda x: (x.Arabic).strip().lower(), axis=1)\n",
        "validation[\"Swahili\"] = validation.Swahili.apply(lambda x: x.lower())\n",
        "\n",
        "validation = validation[[\"Swahili\",\"Arabic\"]]\n",
        "validation.columns = [\"input_text\", \"target_text\"]\n",
        "\n",
        "\n",
        "train.info()\n",
        "validation.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "APIKtxs1O7ix"
      },
      "outputs": [],
      "source": [
        "# For the training set\n",
        "train_txt = \"\\n\".join(train.input_text.values.tolist())\n",
        "file = open(\"Swahili_txt_train.txt\", \"w\")\n",
        "\n",
        "file.write(train_txt)\n",
        "file.close()\n",
        "\n",
        "\n",
        "train_target_txt = \"\\n\".join(train.target_text.values.tolist())\n",
        "file = open(\"Arabic_txt_train.txt\", \"w\")\n",
        "\n",
        "file.write(train_target_txt)\n",
        "file.close()\n",
        "\n",
        "# For the validation set\n",
        "validation_txt = \"\\n\".join(validation.input_text.values.tolist())\n",
        "file = open(\"Swahili_txt_validation.txt\", \"w\")\n",
        "\n",
        "file.write(validation_txt)\n",
        "file.close()\n",
        "\n",
        "\n",
        "validation_target_txt = \"\\n\".join(validation.target_text.values.tolist())\n",
        "file = open(\"Arabic_txt_validation.txt\", \"w\")\n",
        "\n",
        "file.write(validation_target_txt)\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yi_5Go5UeF1E"
      },
      "outputs": [],
      "source": [
        "#BPE Tokenization\n",
        "!python fairseq/scripts/spm_encode.py \\\n",
        "        --model spm.128k.model \\\n",
        "        --output_format=piece \\\n",
        "        --inputs=Arabic_txt_train.txt \\\n",
        "        --outputs=train.ar\n",
        "\n",
        "!python fairseq/scripts/spm_encode.py \\\n",
        "        --model spm.128k.model \\\n",
        "        --output_format=piece \\\n",
        "        --inputs=Swahili_txt_train.txt \\\n",
        "        --outputs=train.sw\n",
        "\n",
        "!python fairseq/scripts/spm_encode.py \\\n",
        "        --model spm.128k.model \\\n",
        "        --output_format=piece \\\n",
        "        --inputs=Arabic_txt_validation.txt \\\n",
        "        --outputs=val.ar\n",
        "!python fairseq/scripts/spm_encode.py \\\n",
        "        --model spm.128k.model \\\n",
        "        --output_format=piece \\\n",
        "        --inputs=Swahili_txt_validation.txt \\\n",
        "        --outputs=val.sw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40ZLJMvT0oZL"
      },
      "outputs": [],
      "source": [
        "! echo $PYTHONPATH\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += \":/content/fairseq/\"\n",
        "\n",
        "! echo $PYTHONPATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8e2oFK4qjKJT"
      },
      "outputs": [],
      "source": [
        "#next is preparing the data to be fed to the transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oH_Lh7WFPGZO"
      },
      "outputs": [],
      "source": [
        "!fairseq-preprocess \\\n",
        "    --source-lang sw --target-lang ar \\\n",
        "    --trainpref train \\\n",
        "    --validpref val \\\n",
        "    --thresholdsrc 0 --thresholdtgt 0 \\\n",
        "    --destdir data_bin \\\n",
        "    --srcdict model_dict.128k.txt --tgtdict model_dict.128k.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvm_M4vCmAwk"
      },
      "source": [
        "# Training\n",
        "The M2M100 model was finetuned starting with the initial downloaded checkpoint : 418M_last_checkpoint.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWhEMFCYec4t"
      },
      "outputs": [],
      "source": [
        "!fairseq-train data_bin \\\n",
        "  --finetune-from-model  \"/content/418M_last_checkpoint.pt\"\\\n",
        "  --save-dir '/content/drive/MyDrive/M2M_FINAL/SWTOAR' \\\n",
        "  --task translation_multi_simple_epoch \\\n",
        "  --encoder-normalize-before \\\n",
        "  --lang-pairs 'sw-ar' \\\n",
        "  --batch-size 16 \\\n",
        "  --decoder-normalize-before \\\n",
        "  --encoder-langtok src \\\n",
        "  --decoder-langtok \\\n",
        "  --criterion cross_entropy \\\n",
        "  --optimizer adafactor \\\n",
        "  --lr-scheduler cosine \\\n",
        "  --lr 3e-05 \\\n",
        "  --max-update 40000 \\\n",
        "  --update-freq 2 \\\n",
        "  --save-interval 1 \\\n",
        "  --save-interval-updates 5000 \\\n",
        "  --keep-interval-updates 10 \\\n",
        "  --no-epoch-checkpoints \\\n",
        "  --log-format simple \\\n",
        "  --log-interval 2 \\\n",
        "  --patience 10 \\\n",
        "  --arch transformer_wmt_en_de_big \\\n",
        "  --encoder-layers 12 --decoder-layers 12 \\\n",
        "  --share-decoder-input-output-embed --share-all-embeddings \\\n",
        "  --ddp-backend no_c10d \\\n",
        "  --max-epoch 15 \\\n",
        "  --wandb-project \"Swahili TO ARABIC M2M\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQUcrY8rmdxx"
      },
      "source": [
        "# Evaluation & Calculating BLEU Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "WoMhPbVXh_y2"
      },
      "outputs": [],
      "source": [
        "# !rm -rf data_bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSTqGqwiQoa7",
        "outputId": "069c91ba-212e-4d06-a797-264c7b24424f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-12-07 23:12:06.724866: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-07 23:12:06.724928: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-07 23:12:06.724966: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-07 23:12:06.735795: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-07 23:12:08.196496: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
            "INFO:fairseq_cli.preprocess:Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', simul_type=None, scoring='bleu', task='translation', source_lang='sw', target_lang='ar', trainpref=None, validpref=None, testpref='val', align_suffix=None, destdir='data_bin', thresholdtgt=0, thresholdsrc=0, tgtdict='data_dict.128k.txt', srcdict='data_dict.128k.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=1, dict_only=False)\n",
            "INFO:fairseq_cli.preprocess:[sw] Dictionary: 128004 types\n",
            "INFO:fairseq_cli.preprocess:[sw] val.sw: 3281 sents, 123931 tokens, 0.000807% replaced (by <unk>)\n",
            "INFO:fairseq_cli.preprocess:[ar] Dictionary: 128004 types\n",
            "INFO:fairseq_cli.preprocess:[ar] val.ar: 3281 sents, 112827 tokens, 0.0% replaced (by <unk>)\n",
            "INFO:fairseq_cli.preprocess:Wrote preprocessed data to data_bin\n"
          ]
        }
      ],
      "source": [
        "!fairseq-preprocess \\\n",
        "    --source-lang sw --target-lang ar \\\n",
        "    --testpref val \\\n",
        "    --thresholdsrc 0 --thresholdtgt 0 \\\n",
        "    --destdir data_bin \\\n",
        "    --srcdict data_dict.128k.txt --tgtdict data_dict.128k.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qh7O9DsYPMkR"
      },
      "outputs": [],
      "source": [
        "#for calculating BLEU score with BPE\n",
        "!fairseq-generate \"data_bin/\"  --batch-size 32 \\\n",
        "      --path \"/content/checkpoint/checkpoint_last.pt\" \\\n",
        "      --fixed-dictionary model_dict.128k.txt \\\n",
        "      -s sw -t ar \\\n",
        "      --beam 5 \\\n",
        "      --task translation_multi_simple_epoch \\\n",
        "      --lang-pairs language_pairs_small_models.txt \\\n",
        "      --decoder-langtok \\\n",
        "      --encoder-langtok src \\\n",
        "      --gen-subset test > outb.txt\n",
        "#for calculating BLEU score without BPE\n",
        "# !fairseq-generate \"data_bin/\"  --batch-size 32 \\\n",
        "#       --path \"/content/checkpoint/checkpoint_last.pt\" \\\n",
        "#       --fixed-dictionary model_dict.128k.txt \\\n",
        "#       -s sw -t ar \\\n",
        "#       --beam 5 \\\n",
        "#       --task translation_multi_simple_epoch \\\n",
        "#       --lang-pairs language_pairs_small_models.txt \\\n",
        "#       --remove-bpe 'sentencepiece' \\\n",
        "#       --decoder-langtok \\\n",
        "#       --encoder-langtok src \\\n",
        "#       --gen-subset test > outb.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYI524V2kZCA"
      },
      "outputs": [],
      "source": [
        "#Without BPE\n",
        "# Generate test with beam=5: BLEU4 = 12.55, 36.8/17.2/9.0/5.0 (BP=0.965, ratio=0.966, syslen=49535, reflen=51302)\n",
        "\n",
        "#With BPE\n",
        "# Generate test with beam=5: BLEU4 = 26.28, 51.5/31.0/21.5/15.1 (BP=0.979, ratio=0.979, syslen=110406, reflen=112742)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
