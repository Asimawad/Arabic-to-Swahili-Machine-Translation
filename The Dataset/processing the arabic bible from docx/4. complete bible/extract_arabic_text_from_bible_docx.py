# -*- coding: utf-8 -*-
"""Extract_Arabic_text_from_bible_docx.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g1sLxh8PoTPZRUQXoof4KwqckTpOtyir

#installing dependencies and unzipping the docx files
"""

!pip install python-docx
!pip install docx2txt
!pip install pyarabic
import os
import docx
import re

!unzip "/content/docx_compressed.zip"
!unzip "/content/all files.zip"
#to remove a directory: !rm -rf docx
# to unzip a compressed file : !unzip "/content/docx_compressed.zip"



#to make a list of files pathes
all_files = os.listdir('/content/docx')
all_files = sorted(all_files)

print(f"no of docx files :{len(all_files)}")

print(all_files)

"""##Merge the docx files

"""

from docx import Document

def combine_word_documents(all_files):
    merged_document = Document()

    for index, file in enumerate(all_files):
        sub_doc = Document(f"/content/docx/{file}")

        # Don't add a page break if you've reached the last file.
        if index < len(all_files)-1:
           sub_doc.add_page_break()

        for element in sub_doc.element.body:
            merged_document.element.body.append(element)

    merged_document.save('/content/ARABIC_COMPLETE.docx')

combine_word_documents(all_files)

"""#view the content of the files

"""

import docx2txt
my_new_text = docx2txt.process("/content/ARABIC_COMPLETE.docx")
#show part of it because its too big
print(my_new_text[:20000])

"""#next step is to remove tashkeel, punctuation, split into verses and remove titles"""

import pyarabic.araby as araby

after_filter1 = araby.strip_diacritics(my_new_text)
import string

def remove_punctuation(text):
    translator = str.maketrans('', '', string.punctuation)
    return text.translate(translator)
# print(after_filter[:900])
after_filter2 = remove_punctuation(after_filter1)
with open("out.txt", "w") as f:
  f.write(after_filter2)

import docx2txt
my_new_text = docx2txt.process("/content/ARABIC_COMPLETE.docx")
#show part of it because its too big
print(my_new_text[:20000])

def split_text_by_word(text, split_word):
    segments = []
    current_segment = []
    capturing = False

    for line in text.split('\n'):
        if split_word in line:
            if not capturing:
                capturing = True
                current_segment = []
            else:
                segments.append('\n'.join(current_segment))
                current_segment = []
        elif capturing:
            current_segment.append(line)

    if current_segment:
        segments.append('\n'.join(current_segment))

    return segments

# Read the text from a file
with open('/content/out.txt', 'r') as file:
    text = file.read()

split_word = "الأصحاح"
split_segments = split_text_by_word(text, split_word)

# Write the segments to separate text files
for idx, segment in enumerate(split_segments):
    with open(f'output/{idx + 1}-segment.txt', 'w') as file:
        file.write(segment)

len(split_segments)

text = "".join(split_segments)

with open("out2.txt", "w") as f:
  f.write(text)

def split_text_by_word(text, split_word):
    segments = []
    current_segment = []
    capturing = False

    for line in text.split('\n'):
        if split_word in line:
            if not capturing:
                capturing = True
                current_segment = []
            else:
                segments.append('\n'.join(current_segment))
                current_segment = []
        elif capturing:
            current_segment.append(line)

    if current_segment:
        segments.append('\n'.join(current_segment))

    return segments

# Read the text from a file
with open('/content/out.txt', 'r') as file:
    text = file.read()

split_word = "المزمور"
split_segments = split_text_by_word(text, split_word)

# # Write the segments to separate text files
# for idx, segment in enumerate(split_segments):
#     with open(f'output/{idx + 1}-segment.txt', 'w') as file:
#         file.write(segment)

