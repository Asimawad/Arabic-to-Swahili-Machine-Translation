{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Installing Dependencies & Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5167,"status":"ok","timestamp":1694047348212,"user":{"displayName":"Asim Emad","userId":"17804099977012185799"},"user_tz":-120},"id":"fTPT1kGAEhMB","outputId":"e0b7dfe2-e1dc-4dc2-8841-43b87edb85bf"},"outputs":[],"source":["!pip install sentencepiece\n","!git clone https://github.com/pytorch/fairseq\n","%cd fairseq\n","!pip install --editable ./\n","%cd /content\n","!pip install fairseq\n","!pip install wandb\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","import pandas as pd\n","import sentencepiece as spm\n","import wandb\n","wandb.login(key=\"YOUR_WANDB_LOGIN_KEY\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Arranging the  files"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":457,"status":"ok","timestamp":1694047373796,"user":{"displayName":"Asim Emad","userId":"17804099977012185799"},"user_tz":-120},"id":"koYV42KWFQni"},"outputs":[],"source":["!mkdir /content/train/\n","!mkdir /content/test/\n","!mkdir /content/val/\n","!mkdir /content/train_bpe/\n","!mkdir /content/test_bpe/\n","!mkdir /content/val_bpe/\n","!mkdir /content/bpe_dict_path\n","%mkdir /content/drive/MyDrive/logs"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Data Preprocessing"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1694047499386,"user":{"displayName":"Asim Emad","userId":"17804099977012185799"},"user_tz":-120},"id":"HI8YwiyRFTsR"},"outputs":[],"source":["!unzip /content/DATA.zip\n","!unzip /content/txt_splitted_files.zip\n","\n","#the entire files\n","All_ar_data = open(\"/content/Arabic.txt\", \"r\").readlines()\n","All_sw_data = open(\"/content/Swahili.txt\",\"r\").readlines()\n","# the training files\n","train_ar_data = open(\"/content/train.ar\", \"r\").readlines()\n","train_sw_data = open(\"/content/train.sw\", \"r\").readlines()\n","# the test files\n","test_ar_data = open(\"/content/test.ar\", \"r\").readlines()\n","test_sw_data = open(\"/content/test.sw\", \"r\").readlines()\n","\n","\n","with open(\"/content/train/train.ar\", \"w\") as fb:\n","    fb.writelines(train_ar_data[2000:])\n","\n","with open(\"/content/train/train.sw\", \"w\") as fb:\n","    fb.writelines(train_sw_data[2000:])\n","\n","with open(\"/content/val/val.ar\", \"w\") as fb:\n","    fb.writelines(train_ar_data[:2000])\n","with open(\"/content/val/val.sw\", \"w\") as fb:\n","    fb.writelines(train_sw_data[:2000])\n","\n","with open(\"/content/test/test.ar\", \"w\") as fb:\n","    fb.writelines(test_ar_data)\n","with open(\"/content/test/test.sw\", \"w\") as fb:\n","    fb.writelines(test_sw_data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#checking the split of the files\n","train_ar_data = open(\"/content/train/train.ar\", \"r\").readlines()\n","test_ar_data = open(\"/content/test/test.ar\", \"r\").readlines()\n","val_ar_data = open(\"/content/val/val.ar\", \"r\").readlines()\n","print(\"For the Arabic files: \\n\")\n","print(len(train_ar_data) , len(test_ar_data) ,len(val_ar_data))\n","\n","#checking the split of the files\n","train_sw_data = open(\"/content/train/train.sw\", \"r\").readlines()\n","test_sw_data = open(\"/content/test/test.sw\", \"r\").readlines()\n","val_sw_data = open(\"/content/val/val.sw\", \"r\").readlines()\n","print(len(train_sw_data) , len(test_sw_data) ,len(val_sw_data))\n"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":13798,"status":"ok","timestamp":1694047519944,"user":{"displayName":"Asim Emad","userId":"17804099977012185799"},"user_tz":-120},"id":"j-LT09DuFbTi"},"outputs":[],"source":["#Training the Bpe models\n","\n","ar_source_file = \"/content/Arabic.txt\"\n","#english training file \"all arabic text\"\n","sw_source_file = \"/content/Swahili.txt\"\n","#swalihi_training file\n","\n","\n","\n","dict_path = \"/content/bpe_dict_path\"\n","#dictionary \"create directory and name it as you like\"\n","ar_train_input = \"/content/train/train.ar\"\n","ar_train_bpe_output = \"/content/train_bpe/train.bpe.ar\"\n","sw_train_input = \"/content/train/train.sw\"\n","sw_train_bpe_output = \"/content/train_bpe/train.bpe.sw\"\n","\n","ar_val_input = \"/content/val/val.ar\"\n","ar_val_bpe_output = \"/content/val_bpe/val.bpe.ar\"\n","sw_val_input = \"/content/val/val.sw\"\n","sw_val_bpe_output = \"/content/val_bpe/val.bpe.sw\"\n","\n","ar_test_input = \"/content/test/test.ar\"\n","ar_test_bpe_output = \"/content/test_bpe/test.bpe.ar\"\n","sw_test_input = \"/content/test/test.sw\"\n","sw_test_bpe_output = \"/content/test_bpe/test.bpe.sw\"\n","\n","vocab_size = 12000\n","\n","\n","\n","\n","#using entire text to find the dictonary for the BPE\n","def train_ar(vocab_size):\n","  model_prefix = dict_path+\"/ar_\" + \"_vocab_\" + str(vocab_size)\n","  spm.SentencePieceTrainer.train(input=ar_source_file\n","      , model_prefix=model_prefix\n","      , vocab_size=vocab_size\n","      , character_coverage = 0.9995\n","      , num_threads=60\n","      , model_type = \"bpe\"\n","      , train_extremely_large_corpus=True\n","  )\n","train_ar(vocab_size)\n","\n","def train_sw(vocab_size):\n","  model_prefix = dict_path + \"/sw_\" + \"_vocab_\" + str(vocab_size)\n","  spm.SentencePieceTrainer.train(input=sw_source_file\n","      , model_prefix=model_prefix\n","      , vocab_size=vocab_size\n","      , character_coverage = 0.9995\n","      , num_threads=60\n","      ,model_type = \"bpe\"\n","      , train_extremely_large_corpus=True\n","  )\n","train_sw(vocab_size)\n","\n","ar_tokenizer = spm.SentencePieceProcessor(model_file=\"/content/bpe_dict_path/ar__vocab_12000.model\")\n","sw_tokenizer = spm.SentencePieceProcessor(model_file=\"/content/bpe_dict_path/sw__vocab_12000.model\")\n","\n","\n","\n","\n","#from the above the BPE models are trained\n","####################################33\n","\n","\n","#lines uses the Arabic BPE model to tokenize the Arabic training data\n","with open(ar_train_input, \"r\", encoding=\"utf-8\") as rf, open(ar_train_bpe_output, \"w\", encoding=\"utf-8\") as wf:\n","    output_lines = []\n","    for line in rf.readlines():\n","        wf.write(' '.join(ar_tokenizer.encode(line, out_type=str)))\n","        wf.write(\"\\n\")\n","\n","\n","with open(sw_train_input, \"r\", encoding=\"utf-8\") as rf, open(sw_train_bpe_output, \"w\", encoding=\"utf-8\") as wf:\n","    output_lines = []\n","    for line in rf.readlines():\n","        wf.write(' '.join(sw_tokenizer.encode(line, out_type=str)))\n","        wf.write(\"\\n\")\n","\n","\n","\n","\n","with open(ar_test_input, \"r\", encoding=\"utf-8\") as rf, open(ar_test_bpe_output, \"w\", encoding=\"utf-8\") as wf:\n","    output_lines = []\n","    for line in rf.readlines():\n","        wf.write(' '.join(ar_tokenizer.encode(line, out_type=str)))\n","        wf.write(\"\\n\")\n","\n","\n","\n","with open(sw_test_input, \"r\", encoding=\"utf-8\") as rf, open(sw_test_bpe_output, \"w\", encoding=\"utf-8\") as wf:\n","    output_lines = []\n","    for line in rf.readlines():\n","        wf.write(' '.join(sw_tokenizer.encode(line, out_type=str)))\n","        wf.write(\"\\n\")\n","\n","\n","with open(ar_val_input, \"r\", encoding=\"utf-8\") as rf, open(ar_val_bpe_output, \"w\", encoding=\"utf-8\") as wf:\n","    output_lines = []\n","    for line in rf.readlines():\n","        wf.write(' '.join(ar_tokenizer.encode(line, out_type=str)))\n","        wf.write(\"\\n\")\n","\n","\n","\n","\n","with open(sw_val_input, \"r\", encoding=\"utf-8\") as rf, open(sw_val_bpe_output, \"w\", encoding=\"utf-8\") as wf:\n","    output_lines = []\n","    for line in rf.readlines():\n","        wf.write(' '.join(sw_tokenizer.encode(line, out_type=str)))\n","        wf.write(\"\\n\")\n","\n"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1694047599142,"user":{"displayName":"Asim Emad","userId":"17804099977012185799"},"user_tz":-120},"id":"ikv_qzd7FbRo"},"outputs":[],"source":["!cut -f1 \"/content/bpe_dict_path/ar__vocab_12000.vocab\" | tail -n +4 | sed \"s/$/ 100/g\" > \"/content/bpe_dict_path/fairseq.ar.vocab\"\n","!cut -f1 \"/content/bpe_dict_path/sw__vocab_12000.vocab\" | tail -n +4 | sed \"s/$/ 100/g\" > \"/content/bpe_dict_path/fairseq.sw.vocab\""]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# FAIRSEQ Preprocess \n","To prepare the data to go into the Transformer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"bVVAOjdQGJeh"},"outputs":[],"source":["SOURCE_LANGUAGE=\"sw\"\n","TARGET_LANGUAGE=\"ar\"\n","TRAIN_PREF=\"/content/train_bpe/train.bpe\"\n","VALID_PREF=\"/content/val_bpe/val.bpe\"\n","TEST_PREF=\"/content/test_bpe/test.bpe\"\n","DEST_DIR=\"/content/sw_ar.tokenized.sw-ar\"\n","SRC_THRES=0\n","TGT_THRES=0\n","ar_DICT_PATH=\"/content/bpe_dict_path/fairseq.ar.vocab\"\n","swahili_DICT_PATH=\"/content/bpe_dict_path/fairseq.sw.vocab\"\n","\n","!fairseq-preprocess    --source-lang $SOURCE_LANGUAGE --target-lang $TARGET_LANGUAGE  \\\n","   --srcdict \"/content/bpe_dict_path/fairseq.sw.vocab\"    --tgtdict \"/content/bpe_dict_path/fairseq.ar.vocab\"  --align-suffix align     --trainpref  $TRAIN_PREF        --validpref $VALID_PREF     --testpref $TEST_PREF   --destdir  $DEST_DIR     --thresholdsrc $SRC_THRES     --thresholdtgt $TGT_THRES"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# FAIRSEQ TRAINING"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_nrrzjB0Geoy","outputId":"5e32885f-4e88-42e3-ddc5-32b06d870436"},"outputs":[],"source":["DROPOUT=0.1\n","ATTENTION_DROPOUT=0\n","ACTIVATION_DROPOUT=0\n","EMBEDDING_SIZE=256\n","ENC_FFNN=1024\n","ENCODER_LAYERS=5\n","ENCODER_ATTENTION_HEADS=8\n","DECODER_LAYERS=5\n","DECODER_ATTENTION_HEADS=8\n","DEC_FFNN=1024\n","EPOCH=550\n","BATCH_SIZE=4096\n","ENCODER_LAYER_DROPOUT=0.1\n","DECODER_LAYER_DROPOUT=0.1\n","# SOURCE_LANGUAGE=swl\n","# TARGET_LANGUAGE=ar\n","LABEL_SMOOTHING=0.1\n","SAVE_DIR=\"/content/drive/MyDrive/checkpoints/\"\n","LABEL_CROSS_ENTROPY=\"label_smoothed_cross_entropy\"\n","WARMUP_UPDATES=4000\n","lEARNING_POLICY=\"inverse_sqrt\"\n","WAND_PROJECT_NAME=\"LARGE_SCALE_AR_TO_SWAHILI_Translation\"\n","\n","!fairseq-train \"/content/sw_ar.tokenized.sw-ar\" \\\n","    --arch transformer \\\n","    --dropout $DROPOUT \\\n","    --attention-dropout 0     --encoder-embed-dim $EMBEDDING_SIZE \\\n","    --encoder-ffn-embed-dim $ENC_FFNN  \\\n","    --encoder-layers $ENCODER_LAYERS  \\\n","    --encoder-attention-heads $ENCODER_ATTENTION_HEADS \\\n","    --encoder-learned-pos  \\\n","    --decoder-embed-dim $EMBEDDING_SIZE \\\n","    --decoder-ffn-embed-dim $DEC_FFNN  \\\n","    --decoder-layers $DECODER_LAYERS \\\n","    --decoder-attention-heads $DECODER_ATTENTION_HEADS \\\n","    --decoder-learned-pos   \\\n","    --max-epoch $EPOCH  \\\n","    --optimizer adam \\\n","    --lr 5e-4 \\\n","    --max-tokens $BATCH_SIZE \\\n","    --seed 1     --encoder-layerdrop $ENCODER_LAYER_DROPOUT     --decoder-layerdrop $DECODER_LAYER_DROPOUT \\\n","    --criterion $LABEL_CROSS_ENTROPY     --warmup-updates $WARMUP_UPDATES \\\n","    --source-lang sw    --label-smoothing $LABEL_SMOOTHING \\\n","    --lr-scheduler $lEARNING_POLICY   --save-dir $SAVE_DIR \\\n","    --find-unused-parameters  \\\n","    --target-lang ar \\\n","    --activation-dropout $ACTIVATION_DROPOUT  \\\n","    --ddp-backend=no_c10d \\\n","    --no-epoch-checkpoints --wandb-project $WAND_PROJECT_NAME \\\n","    --log-format=json --log-interval=10 2>&1    |  tee  \"/content/drive/MyDrive/logs/training_log.log\""]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Generating Translation from the test set & Calculating BLEU Score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SfLRUsrvJga3","outputId":"1ea72e2b-b1aa-428b-88a6-a46d24f65cbc"},"outputs":[],"source":["BATCH_SIZE = 128\n","BEAM=5\n","SEED=1\n","SCORING=\"bleu\"\n","CHECKPOINT_PATH= f\"{SAVE_DIR}checkpoint_last.pt\"\n","#to calculate the BLEU score with bpe tkenization, remove the \"    --remove-bpe 'sentencepiece' \\ \" argument .\n","!fairseq-generate \"/content/sw_ar.tokenized.sw-ar\"\\\n","    --batch-size $BATCH_SIZE \\\n","    --beam $BEAM \\\n","    --path $CHECKPOINT_PATH \\\n","    --remove-bpe 'sentencepiece' \\ \n","    --seed $SEED \\\n","    --scoring bleu > \"/content/drive/MyDrive/checkpoints/latest_result.txt\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hg6jrdTLJgSv"},"outputs":[],"source":["#To print the bleu scorre from within the file\n","with open( \"/content/drive/MyDrive/checkpoints/latest_result.txt\" ) as f:\n","  text = f.read()\n","  text = text.split()\n","  print(\" \".join(text[-8:-5]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3wT0Pa7EJgKA"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BSbs52kSJftJ"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
